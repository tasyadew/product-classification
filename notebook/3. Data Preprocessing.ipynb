{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c1a1f1-bb3d-44d1-9e0d-fb34696dcd5c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c195396-0a06-4cef-9275-3e8cac5560b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\penguin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ba3c7c7-71e1-46b0-826d-814c4965c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = joblib.load(\"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_train.pkl\")\n",
    "y_train = joblib.load(\"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\y_train.pkl\")\n",
    "\n",
    "X_test = joblib.load(\"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_test.pkl\")\n",
    "y_test = joblib.load(\"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\y_test.pkl\")\n",
    "\n",
    "X_valid = joblib.load(\"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_valid.pkl\")\n",
    "y_valid = joblib.load(\"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\y_valid.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44ecb653-932c-47d6-b67e-dd49905edef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\penguin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stopwords from the NLTK library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(df, column_name):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x).lower())\n",
    "    \n",
    "    # Remove stopwords and join the words with a single space\n",
    "    df[column_name] = df[column_name].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb2500d4-1ecb-4f12-858d-3115b6860127",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_text(X_train, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87b6901d-750a-4ec1-b864-8a82b3655565",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess_text(X_test, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcbd1284-d13d-4fa4-a200-c9e8b6650e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = preprocess_text(X_valid, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91c4c55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13454</th>\n",
       "      <td>eurosafe trailing socket neon surge 5m green e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14181</th>\n",
       "      <td>la boheme classic ring donut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>topvalu himalaya rock salt pouch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12466</th>\n",
       "      <td>russell taylors air fryer touch screen z4c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>ot cookies chocolate chip raisin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>lazat springroll vegetable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13418</th>\n",
       "      <td>baba microgreen cultivation box mt3003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>marina chicken fries black pepper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>knorr ikan bilis seasoning powder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>le cafe master roast columbia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11408 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title\n",
       "13454  eurosafe trailing socket neon surge 5m green e...\n",
       "14181                       la boheme classic ring donut\n",
       "819                     topvalu himalaya rock salt pouch\n",
       "12466         russell taylors air fryer touch screen z4c\n",
       "4609                    ot cookies chocolate chip raisin\n",
       "...                                                  ...\n",
       "5191                          lazat springroll vegetable\n",
       "13418             baba microgreen cultivation box mt3003\n",
       "5390                   marina chicken fries black pepper\n",
       "860                    knorr ikan bilis seasoning powder\n",
       "7270                       le cafe master roast columbia\n",
       "\n",
       "[11408 rows x 1 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0dec50b-eeea-483c-9b69-51c0ee50e0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_vectorizer_model.pkl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(X_train['title'])\n",
    "\n",
    "X_train_bow = count_vectorizer.transform(X_train['title'])\n",
    "X_test_bow = count_vectorizer.transform(X_test['title'])\n",
    "X_valid_bow = count_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "joblib.dump(count_vectorizer, 'count_vectorizer_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0f585be-1e3c-47c1-ba36-6890fb43cbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer_model.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_vectorizer.fit(X_train['title'])\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train['title'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid['title'])\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer_model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "533acafd-9a6d-443c-a707-34a2c2f3667c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11408x7630 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 55676 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fe5feec-c3cf-41ff-b965-4310de4e4feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11408x7630 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55676 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "975c4259-f6b9-405c-aa4d-58e7aa52eaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13454                   Hardware\n",
       "14181                      Bread\n",
       "819      Spices & Dry condiments\n",
       "12466                  Appliance\n",
       "4609           Biscuit & Cookies\n",
       "                  ...           \n",
       "5191                 Frozen food\n",
       "13418                  Gardening\n",
       "5390                 Frozen food\n",
       "860      Spices & Dry condiments\n",
       "7270                      Coffee\n",
       "Name: Category, Length: 11408, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cae5763-b9b1-4dab-87e3-26fe43f0ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "y_valid_encoded = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4948f63d-ecc2-4f83-86ce-1898f3ffa22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\y_test_encoded.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_train_bow, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_train_bow.pkl\")\n",
    "joblib.dump(y_train_encoded, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\y_train_encoded.pkl\")\n",
    "joblib.dump(X_valid_bow, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_valid_bow.pkl\")\n",
    "joblib.dump(y_valid_encoded, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\y_valid_encoded.pkl\")\n",
    "joblib.dump(X_test_bow, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_test_bow.pkl\")\n",
    "joblib.dump(y_test_encoded, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\y_test_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1da5ec7e-ec09-4fb5-ac11-1320b250497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_test_tfidf.pkl']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_train_tfidf, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_train_tfidf.pkl\")\n",
    "joblib.dump(X_valid_tfidf, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_valid_tfidf.pkl\")\n",
    "joblib.dump(X_test_tfidf, \"C:\\\\Users\\\\penguin\\\\code\\\\product-classification\\\\dataset\\\\2 - processed\\\\X_test_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8f70028-54da-4182-a400-18f3b493c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of decoded values for test data:\n",
      "['Laundry' 'Pet food' 'Home interior' 'Spices & Dry condiments' 'Laundry'\n",
      " 'Chips & Crisps' 'Dish washing' 'Car care & Accessories' 'Sanitary' 'Tea']\n",
      "Example of decoded values for validation data:\n",
      "['Baby needs' 'Sauce & Paste' 'Bath & Towel' 'Cosmetic' 'Sanitary'\n",
      " 'Kitchen utensil' 'Spices & Dry condiments' 'Sauce & Paste'\n",
      " 'Kitchen utensil' 'Bread']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_decoded = le.inverse_transform(y_test_encoded)\n",
    "\n",
    "y_valid_decoded = le.inverse_transform(y_valid_encoded)\n",
    "\n",
    "print(\"Example of decoded values for test data:\")\n",
    "print(y_test_decoded[:10]) \n",
    "\n",
    "print(\"Example of decoded values for validation data:\")\n",
    "print(y_valid_decoded[:10])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77b63b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Laundry' 'Pet food' 'Home interior' ... 'Chocolate & Candy'\n",
      " 'Butter & Creams' 'Sanitary']\n"
     ]
    }
   ],
   "source": [
    "print(y_test_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f51b786e-4d11-41bd-a158-20f78d858b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train_bow, y_train_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b881780-0a79-46e0-bba3-a369b732ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred_test = model.predict(X_test_bow)  \n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_test)\n",
    "report = classification_report(y_test_encoded, y_pred_test)\n",
    "\n",
    "y_pred_valid = model.predict(X_valid_bow) \n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "report_valid = classification_report(y_valid_encoded, y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f8c22b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \"Air fresheners\",\n",
      "1: \"Appliance\",\n",
      "2: \"Baby needs\",\n",
      "3: \"Bath & Lotion\",\n",
      "4: \"Bath & Towel\",\n",
      "5: \"Bedding\",\n",
      "6: \"Biscuit & Cookies\",\n",
      "7: \"Body care\",\n",
      "8: \"Bread\",\n",
      "9: \"Butter & Creams\",\n",
      "10: \"Car care & Accessories\",\n",
      "11: \"Carbonated & Packed drink\",\n",
      "12: \"Cereal & Breakfast\",\n",
      "13: \"Cheese\",\n",
      "14: \"Chips & Crisps\",\n",
      "15: \"Chocolate & Candy\",\n",
      "16: \"Chocolate & Nutritious drink\",\n",
      "17: \"Cleaning & Tools\",\n",
      "18: \"Coffee\",\n",
      "19: \"Cooking & Baking ingredients\",\n",
      "20: \"Cooking oil\",\n",
      "21: \"Cosmetic\",\n",
      "22: \"Cutlery\",\n",
      "23: \"Deodorants\",\n",
      "24: \"Desserts & Ice cream\",\n",
      "25: \"Diapers & Wipes\",\n",
      "26: \"Dinnerware\",\n",
      "27: \"Dish washing\",\n",
      "28: \"Drinkware\",\n",
      "29: \"Dry & Canned food\",\n",
      "30: \"Eggs\",\n",
      "31: \"Fish & Seafood\",\n",
      "32: \"Food\",\n",
      "33: \"Food storage\",\n",
      "34: \"Frozen food\",\n",
      "35: \"Fruits\",\n",
      "36: \"Gardening\",\n",
      "37: \"Hair care\",\n",
      "38: \"Hand care\",\n",
      "39: \"Hardware\",\n",
      "40: \"Health & Safety\",\n",
      "41: \"Health & Wellness\",\n",
      "42: \"Home interior\",\n",
      "43: \"Juices\",\n",
      "44: \"Juices & Cordial\",\n",
      "45: \"Kitchen organiser\",\n",
      "46: \"Kitchen utensil\",\n",
      "47: \"Laundry\",\n",
      "48: \"Meat & Poultry\",\n",
      "49: \"Milk\",\n",
      "50: \"Milk & Creamers\",\n",
      "51: \"Milk powder\",\n",
      "52: \"Non halal\",\n",
      "53: \"Noodles\",\n",
      "54: \"Noodles & Pasta\",\n",
      "55: \"Nuts & Seeds\",\n",
      "56: \"Oral care\",\n",
      "57: \"Pest control\",\n",
      "58: \"Pet food\",\n",
      "59: \"Pot & Pan\",\n",
      "60: \"Preserve foods\",\n",
      "61: \"Rice and grains\",\n",
      "62: \"Sanitary\",\n",
      "63: \"Sauce & Paste\",\n",
      "64: \"Shaving & Grooming\",\n",
      "65: \"Skin care\",\n",
      "66: \"Spices & Dry condiments\",\n",
      "67: \"Sports & Outdoor\",\n",
      "68: \"Stationery\",\n",
      "69: \"Sugars & Sweeteners\",\n",
      "70: \"Tea\",\n",
      "71: \"Tissue\",\n",
      "72: \"Tofu\",\n",
      "73: \"Toys\",\n",
      "74: \"Vegetable\",\n",
      "75: \"Water\",\n",
      "76: \"Yogurt & Pudding\",\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the mapping of numerical values to categories\n",
    "category_mapping = {index: label for index, label in enumerate(le.classes_)}\n",
    "\n",
    "# Print the category mapping\n",
    "for numerical_value, category in category_mapping.items():\n",
    "    print(f'{numerical_value}: \"{category}\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aeb28e41-0185-4875-9640-4237e11294d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89        24\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       0.96      0.96      0.96        25\n",
      "           3       1.00      0.75      0.86         8\n",
      "           4       1.00      0.96      0.98        27\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.73      0.77      0.75        53\n",
      "           7       0.97      0.90      0.94        42\n",
      "           8       1.00      0.88      0.93         8\n",
      "           9       0.90      0.75      0.82        12\n",
      "          10       0.78      0.64      0.70        11\n",
      "          11       0.64      0.51      0.57        41\n",
      "          12       0.83      0.80      0.82        50\n",
      "          13       0.77      1.00      0.87        17\n",
      "          14       0.95      0.77      0.85        47\n",
      "          15       0.88      0.84      0.86        82\n",
      "          16       0.64      0.64      0.64        11\n",
      "          17       0.88      0.75      0.81        48\n",
      "          18       0.84      0.86      0.85        36\n",
      "          19       0.79      0.77      0.78        30\n",
      "          20       0.79      0.94      0.86        16\n",
      "          21       1.00      0.75      0.86         4\n",
      "          22       1.00      0.50      0.67         4\n",
      "          23       0.80      0.67      0.73        12\n",
      "          24       0.94      0.76      0.84        21\n",
      "          25       0.94      1.00      0.97        16\n",
      "          26       0.86      0.90      0.88        21\n",
      "          27       0.64      0.88      0.74         8\n",
      "          28       0.90      0.96      0.93        28\n",
      "          29       0.82      0.72      0.77        39\n",
      "          30       0.75      0.75      0.75         4\n",
      "          31       0.00      0.00      0.00         3\n",
      "          32       1.00      0.86      0.92         7\n",
      "          33       0.93      1.00      0.97        14\n",
      "          34       0.68      0.82      0.74        71\n",
      "          35       0.67      0.50      0.57         8\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       0.82      0.92      0.87        25\n",
      "          38       0.70      1.00      0.82         7\n",
      "          39       1.00      0.78      0.88         9\n",
      "          40       0.62      0.83      0.71         6\n",
      "          41       1.00      0.22      0.36         9\n",
      "          42       0.97      0.94      0.95        31\n",
      "          43       0.83      0.71      0.77         7\n",
      "          44       0.67      0.88      0.76        16\n",
      "          45       0.75      0.86      0.80         7\n",
      "          46       1.00      0.65      0.79        17\n",
      "          47       1.00      0.97      0.99        38\n",
      "          48       0.33      0.20      0.25         5\n",
      "          49       0.40      0.50      0.44         8\n",
      "          50       0.71      0.69      0.70        29\n",
      "          51       0.87      0.87      0.87        23\n",
      "          52       0.60      0.27      0.37        11\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.81      0.81      0.81        63\n",
      "          55       0.78      0.82      0.80        22\n",
      "          56       1.00      0.91      0.96        35\n",
      "          57       1.00      0.69      0.81        16\n",
      "          58       0.96      0.73      0.83        30\n",
      "          59       0.56      0.62      0.59         8\n",
      "          60       1.00      0.57      0.73         7\n",
      "          61       0.75      0.75      0.75         8\n",
      "          62       0.97      1.00      0.98        29\n",
      "          63       0.85      0.88      0.86        90\n",
      "          64       0.36      0.45      0.40        11\n",
      "          65       0.88      0.88      0.88        34\n",
      "          66       0.73      0.73      0.73        49\n",
      "          67       0.29      0.50      0.36         4\n",
      "          68       0.87      0.92      0.89        71\n",
      "          69       0.44      0.67      0.53         6\n",
      "          70       0.83      0.80      0.82        25\n",
      "          71       0.88      1.00      0.93        14\n",
      "          72       1.00      1.00      1.00         2\n",
      "          73       1.00      0.67      0.80        12\n",
      "          74       0.21      0.82      0.33        17\n",
      "          75       1.00      0.50      0.67         2\n",
      "          76       1.00      0.87      0.93        31\n",
      "\n",
      "    accuracy                           0.81      1711\n",
      "   macro avg       0.80      0.75      0.76      1711\n",
      "weighted avg       0.84      0.81      0.82      1711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50b7def5-defe-46d8-a82c-c92c41544fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        16\n",
      "           1       0.88      0.78      0.82         9\n",
      "           2       1.00      0.94      0.97        16\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       0.94      0.89      0.91        18\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.73      0.91      0.81        35\n",
      "           7       1.00      0.86      0.92        28\n",
      "           8       1.00      0.80      0.89         5\n",
      "           9       1.00      0.75      0.86         8\n",
      "          10       1.00      0.71      0.83         7\n",
      "          11       0.56      0.56      0.56        27\n",
      "          12       0.79      0.79      0.79        33\n",
      "          13       0.86      1.00      0.92        12\n",
      "          14       0.81      0.81      0.81        32\n",
      "          15       0.74      0.62      0.67        55\n",
      "          16       0.33      0.14      0.20         7\n",
      "          17       0.80      0.85      0.82        33\n",
      "          18       0.78      0.84      0.81        25\n",
      "          19       0.89      0.85      0.87        20\n",
      "          20       0.77      1.00      0.87        10\n",
      "          21       1.00      0.50      0.67         2\n",
      "          22       1.00      0.50      0.67         2\n",
      "          23       1.00      1.00      1.00         8\n",
      "          24       0.88      0.50      0.64        14\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       0.92      0.79      0.85        14\n",
      "          27       0.71      1.00      0.83         5\n",
      "          28       0.94      0.89      0.91        18\n",
      "          29       0.69      0.69      0.69        26\n",
      "          30       1.00      0.50      0.67         2\n",
      "          31       0.50      1.00      0.67         2\n",
      "          32       1.00      0.80      0.89         5\n",
      "          33       0.90      1.00      0.95         9\n",
      "          34       0.78      0.73      0.75        48\n",
      "          35       0.80      0.80      0.80         5\n",
      "          36       1.00      1.00      1.00         4\n",
      "          37       0.86      0.75      0.80        16\n",
      "          38       0.67      0.40      0.50         5\n",
      "          39       0.83      0.83      0.83         6\n",
      "          40       0.67      0.50      0.57         4\n",
      "          41       0.50      0.50      0.50         6\n",
      "          42       0.94      0.81      0.87        21\n",
      "          43       0.80      0.80      0.80         5\n",
      "          44       0.75      0.82      0.78        11\n",
      "          45       0.67      0.40      0.50         5\n",
      "          46       0.78      0.64      0.70        11\n",
      "          47       0.96      0.88      0.92        26\n",
      "          48       0.50      0.33      0.40         3\n",
      "          49       0.57      0.80      0.67         5\n",
      "          50       0.62      0.50      0.56        20\n",
      "          51       0.92      0.73      0.81        15\n",
      "          52       1.00      0.50      0.67         8\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.85      0.79      0.82        43\n",
      "          55       0.69      0.79      0.73        14\n",
      "          56       0.81      0.91      0.86        23\n",
      "          57       0.88      0.70      0.78        10\n",
      "          58       1.00      0.85      0.92        20\n",
      "          59       0.67      0.40      0.50         5\n",
      "          60       1.00      0.75      0.86         4\n",
      "          61       0.50      0.20      0.29         5\n",
      "          62       1.00      0.95      0.97        19\n",
      "          63       0.81      0.85      0.83        60\n",
      "          64       0.60      0.43      0.50         7\n",
      "          65       0.74      0.87      0.80        23\n",
      "          66       0.77      0.82      0.79        33\n",
      "          67       0.60      1.00      0.75         3\n",
      "          68       0.89      1.00      0.94        47\n",
      "          69       0.43      0.75      0.55         4\n",
      "          70       0.75      0.94      0.83        16\n",
      "          71       1.00      0.90      0.95        10\n",
      "          72       1.00      1.00      1.00         1\n",
      "          73       0.83      0.62      0.71         8\n",
      "          74       0.18      0.75      0.29        12\n",
      "          75       1.00      1.00      1.00         2\n",
      "          76       0.95      0.86      0.90        21\n",
      "\n",
      "    accuracy                           0.79      1141\n",
      "   macro avg       0.80      0.75      0.76      1141\n",
      "weighted avg       0.82      0.79      0.80      1141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c166988b-6b0c-46fe-a2fb-4006622fac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train_tfidf, y_train_encoded)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1c3a769-dcc1-4bbb-adf0-4b4806e7c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred_test = model.predict(X_test_tfidf)  \n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_test)\n",
    "report = classification_report(y_test_encoded, y_pred_test)\n",
    "\n",
    "y_pred_valid = model.predict(X_valid_tfidf) \n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "report_valid = classification_report(y_valid_encoded, y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "860a3d8a-fccc-4ef1-a591-5d8111fca757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        24\n",
      "           1       0.92      0.85      0.88        13\n",
      "           2       0.85      0.88      0.86        25\n",
      "           3       0.58      0.88      0.70         8\n",
      "           4       0.86      0.93      0.89        27\n",
      "           5       0.90      0.90      0.90        10\n",
      "           6       0.64      0.70      0.67        53\n",
      "           7       0.95      0.88      0.91        42\n",
      "           8       1.00      0.88      0.93         8\n",
      "           9       0.75      0.75      0.75        12\n",
      "          10       0.78      0.64      0.70        11\n",
      "          11       0.61      0.49      0.54        41\n",
      "          12       0.78      0.80      0.79        50\n",
      "          13       0.76      0.94      0.84        17\n",
      "          14       0.83      0.74      0.79        47\n",
      "          15       0.91      0.76      0.83        82\n",
      "          16       0.55      0.55      0.55        11\n",
      "          17       0.80      0.83      0.82        48\n",
      "          18       0.84      0.86      0.85        36\n",
      "          19       0.80      0.80      0.80        30\n",
      "          20       0.69      0.69      0.69        16\n",
      "          21       1.00      0.25      0.40         4\n",
      "          22       0.67      0.50      0.57         4\n",
      "          23       0.88      0.58      0.70        12\n",
      "          24       0.94      0.76      0.84        21\n",
      "          25       0.94      0.94      0.94        16\n",
      "          26       0.75      0.86      0.80        21\n",
      "          27       0.67      0.75      0.71         8\n",
      "          28       0.96      0.96      0.96        28\n",
      "          29       0.84      0.69      0.76        39\n",
      "          30       1.00      0.50      0.67         4\n",
      "          31       0.00      0.00      0.00         3\n",
      "          32       1.00      0.71      0.83         7\n",
      "          33       0.88      1.00      0.93        14\n",
      "          34       0.78      0.79      0.78        71\n",
      "          35       0.71      0.62      0.67         8\n",
      "          36       0.83      1.00      0.91         5\n",
      "          37       0.69      0.88      0.77        25\n",
      "          38       1.00      1.00      1.00         7\n",
      "          39       1.00      0.89      0.94         9\n",
      "          40       0.83      0.83      0.83         6\n",
      "          41       0.10      0.56      0.17         9\n",
      "          42       0.88      0.90      0.89        31\n",
      "          43       0.50      0.43      0.46         7\n",
      "          44       0.64      0.88      0.74        16\n",
      "          45       0.67      0.57      0.62         7\n",
      "          46       0.79      0.65      0.71        17\n",
      "          47       0.85      0.89      0.87        38\n",
      "          48       0.50      0.40      0.44         5\n",
      "          49       0.36      0.50      0.42         8\n",
      "          50       0.53      0.62      0.57        29\n",
      "          51       0.89      0.74      0.81        23\n",
      "          52       0.50      0.18      0.27        11\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.81      0.75      0.78        63\n",
      "          55       0.70      0.86      0.78        22\n",
      "          56       0.97      0.91      0.94        35\n",
      "          57       0.92      0.75      0.83        16\n",
      "          58       0.85      0.73      0.79        30\n",
      "          59       0.67      0.50      0.57         8\n",
      "          60       1.00      0.43      0.60         7\n",
      "          61       0.88      0.88      0.88         8\n",
      "          62       0.97      1.00      0.98        29\n",
      "          63       0.82      0.87      0.84        90\n",
      "          64       0.38      0.55      0.44        11\n",
      "          65       0.85      0.68      0.75        34\n",
      "          66       0.70      0.76      0.73        49\n",
      "          67       0.50      0.50      0.50         4\n",
      "          68       0.86      0.92      0.88        71\n",
      "          69       0.67      0.67      0.67         6\n",
      "          70       0.81      0.84      0.82        25\n",
      "          71       0.71      0.71      0.71        14\n",
      "          72       1.00      1.00      1.00         2\n",
      "          73       1.00      0.75      0.86        12\n",
      "          74       0.61      0.65      0.63        17\n",
      "          75       1.00      0.50      0.67         2\n",
      "          76       1.00      0.87      0.93        31\n",
      "\n",
      "    accuracy                           0.78      1711\n",
      "   macro avg       0.77      0.72      0.73      1711\n",
      "weighted avg       0.80      0.78      0.79      1711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b751cd57-a972-4f25-ac1a-f6c80ef34b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.90        16\n",
      "           1       0.89      0.89      0.89         9\n",
      "           2       0.93      0.81      0.87        16\n",
      "           3       0.60      0.60      0.60         5\n",
      "           4       0.94      0.94      0.94        18\n",
      "           5       1.00      0.71      0.83         7\n",
      "           6       0.64      0.80      0.71        35\n",
      "           7       0.96      0.89      0.93        28\n",
      "           8       1.00      0.80      0.89         5\n",
      "           9       0.50      0.50      0.50         8\n",
      "          10       1.00      0.57      0.73         7\n",
      "          11       0.58      0.56      0.57        27\n",
      "          12       0.84      0.82      0.83        33\n",
      "          13       0.79      0.92      0.85        12\n",
      "          14       0.79      0.84      0.82        32\n",
      "          15       0.76      0.67      0.71        55\n",
      "          16       0.40      0.29      0.33         7\n",
      "          17       0.84      0.79      0.81        33\n",
      "          18       0.81      0.88      0.85        25\n",
      "          19       0.83      0.75      0.79        20\n",
      "          20       0.70      0.70      0.70        10\n",
      "          21       1.00      0.50      0.67         2\n",
      "          22       1.00      0.50      0.67         2\n",
      "          23       1.00      0.88      0.93         8\n",
      "          24       0.67      0.43      0.52        14\n",
      "          25       0.91      0.91      0.91        11\n",
      "          26       0.77      0.71      0.74        14\n",
      "          27       0.62      1.00      0.77         5\n",
      "          28       0.89      0.89      0.89        18\n",
      "          29       0.77      0.77      0.77        26\n",
      "          30       1.00      0.50      0.67         2\n",
      "          31       1.00      0.50      0.67         2\n",
      "          32       1.00      1.00      1.00         5\n",
      "          33       0.90      1.00      0.95         9\n",
      "          34       0.80      0.67      0.73        48\n",
      "          35       0.80      0.80      0.80         5\n",
      "          36       0.80      1.00      0.89         4\n",
      "          37       0.87      0.81      0.84        16\n",
      "          38       0.67      0.40      0.50         5\n",
      "          39       0.67      0.67      0.67         6\n",
      "          40       1.00      0.50      0.67         4\n",
      "          41       0.07      0.50      0.12         6\n",
      "          42       0.91      0.95      0.93        21\n",
      "          43       0.83      1.00      0.91         5\n",
      "          44       0.90      0.82      0.86        11\n",
      "          45       1.00      0.40      0.57         5\n",
      "          46       0.75      0.55      0.63        11\n",
      "          47       0.96      0.85      0.90        26\n",
      "          48       0.50      0.33      0.40         3\n",
      "          49       0.36      0.80      0.50         5\n",
      "          50       0.56      0.45      0.50        20\n",
      "          51       0.85      0.73      0.79        15\n",
      "          52       0.80      0.50      0.62         8\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.94      0.79      0.86        43\n",
      "          55       0.71      0.71      0.71        14\n",
      "          56       0.92      1.00      0.96        23\n",
      "          57       1.00      0.70      0.82        10\n",
      "          58       0.89      0.85      0.87        20\n",
      "          59       0.67      0.40      0.50         5\n",
      "          60       1.00      0.75      0.86         4\n",
      "          61       0.33      0.20      0.25         5\n",
      "          62       1.00      1.00      1.00        19\n",
      "          63       0.77      0.83      0.80        60\n",
      "          64       0.67      0.57      0.62         7\n",
      "          65       0.73      0.70      0.71        23\n",
      "          66       0.73      0.91      0.81        33\n",
      "          67       0.60      1.00      0.75         3\n",
      "          68       0.92      0.98      0.95        47\n",
      "          69       0.43      0.75      0.55         4\n",
      "          70       0.74      0.88      0.80        16\n",
      "          71       0.88      0.70      0.78        10\n",
      "          72       1.00      1.00      1.00         1\n",
      "          73       0.83      0.62      0.71         8\n",
      "          74       0.46      0.50      0.48        12\n",
      "          75       1.00      1.00      1.00         2\n",
      "          76       0.85      0.81      0.83        21\n",
      "\n",
      "    accuracy                           0.77      1141\n",
      "   macro avg       0.79      0.72      0.74      1141\n",
      "weighted avg       0.81      0.77      0.78      1141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d7100788-0a9e-4b17-b9fb-cac98f16887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "Test Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89        24\n",
      "           1       0.92      0.85      0.88        13\n",
      "           2       0.96      0.92      0.94        25\n",
      "           3       1.00      0.75      0.86         8\n",
      "           4       1.00      0.93      0.96        27\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       0.72      0.74      0.73        53\n",
      "           7       0.97      0.88      0.93        42\n",
      "           8       1.00      0.88      0.93         8\n",
      "           9       0.90      0.75      0.82        12\n",
      "          10       0.75      0.82      0.78        11\n",
      "          11       0.27      0.51      0.35        41\n",
      "          12       0.78      0.78      0.78        50\n",
      "          13       0.81      1.00      0.89        17\n",
      "          14       0.85      0.70      0.77        47\n",
      "          15       0.93      0.84      0.88        82\n",
      "          16       0.53      0.82      0.64        11\n",
      "          17       0.84      0.79      0.82        48\n",
      "          18       0.84      0.86      0.85        36\n",
      "          19       0.86      0.80      0.83        30\n",
      "          20       0.68      0.81      0.74        16\n",
      "          21       1.00      0.50      0.67         4\n",
      "          22       1.00      0.50      0.67         4\n",
      "          23       0.80      0.67      0.73        12\n",
      "          24       0.94      0.71      0.81        21\n",
      "          25       0.94      0.94      0.94        16\n",
      "          26       0.83      0.90      0.86        21\n",
      "          27       0.50      0.75      0.60         8\n",
      "          28       0.96      0.93      0.95        28\n",
      "          29       0.81      0.67      0.73        39\n",
      "          30       1.00      0.50      0.67         4\n",
      "          31       0.00      0.00      0.00         3\n",
      "          32       1.00      0.57      0.73         7\n",
      "          33       0.74      1.00      0.85        14\n",
      "          34       0.71      0.79      0.75        71\n",
      "          35       0.75      0.38      0.50         8\n",
      "          36       0.83      1.00      0.91         5\n",
      "          37       0.77      0.92      0.84        25\n",
      "          38       0.88      1.00      0.93         7\n",
      "          39       1.00      0.78      0.88         9\n",
      "          40       0.62      0.83      0.71         6\n",
      "          41       0.50      0.22      0.31         9\n",
      "          42       0.94      0.97      0.95        31\n",
      "          43       0.71      0.71      0.71         7\n",
      "          44       0.70      0.88      0.78        16\n",
      "          45       0.75      0.86      0.80         7\n",
      "          46       1.00      0.71      0.83        17\n",
      "          47       1.00      0.92      0.96        38\n",
      "          48       0.67      0.40      0.50         5\n",
      "          49       0.27      0.50      0.35         8\n",
      "          50       0.54      0.66      0.59        29\n",
      "          51       0.79      0.83      0.81        23\n",
      "          52       0.67      0.36      0.47        11\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.75      0.81      0.78        63\n",
      "          55       0.78      0.82      0.80        22\n",
      "          56       1.00      0.86      0.92        35\n",
      "          57       0.92      0.69      0.79        16\n",
      "          58       1.00      0.80      0.89        30\n",
      "          59       0.71      0.62      0.67         8\n",
      "          60       1.00      0.57      0.73         7\n",
      "          61       0.88      0.88      0.88         8\n",
      "          62       0.93      0.97      0.95        29\n",
      "          63       0.85      0.88      0.86        90\n",
      "          64       0.38      0.55      0.44        11\n",
      "          65       0.75      0.62      0.68        34\n",
      "          66       0.67      0.73      0.70        49\n",
      "          67       0.67      0.50      0.57         4\n",
      "          68       0.86      0.90      0.88        71\n",
      "          69       0.44      0.67      0.53         6\n",
      "          70       0.74      0.68      0.71        25\n",
      "          71       0.87      0.93      0.90        14\n",
      "          72       1.00      1.00      1.00         2\n",
      "          73       0.88      0.58      0.70        12\n",
      "          74       0.67      0.59      0.62        17\n",
      "          75       1.00      0.50      0.67         2\n",
      "          76       1.00      0.87      0.93        31\n",
      "\n",
      "    accuracy                           0.79      1711\n",
      "   macro avg       0.79      0.74      0.75      1711\n",
      "weighted avg       0.82      0.79      0.80      1711\n",
      "\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.90        16\n",
      "           1       0.88      0.78      0.82         9\n",
      "           2       1.00      0.88      0.93        16\n",
      "           3       0.80      0.80      0.80         5\n",
      "           4       0.94      0.83      0.88        18\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.71      0.86      0.78        35\n",
      "           7       0.96      0.82      0.88        28\n",
      "           8       1.00      0.80      0.89         5\n",
      "           9       1.00      0.75      0.86         8\n",
      "          10       1.00      0.86      0.92         7\n",
      "          11       0.25      0.52      0.34        27\n",
      "          12       0.85      0.85      0.85        33\n",
      "          13       0.71      1.00      0.83        12\n",
      "          14       0.81      0.69      0.75        32\n",
      "          15       0.75      0.65      0.70        55\n",
      "          16       0.38      0.43      0.40         7\n",
      "          17       0.86      0.76      0.81        33\n",
      "          18       0.79      0.88      0.83        25\n",
      "          19       0.80      0.80      0.80        20\n",
      "          20       0.64      0.90      0.75        10\n",
      "          21       1.00      0.50      0.67         2\n",
      "          22       1.00      0.50      0.67         2\n",
      "          23       1.00      1.00      1.00         8\n",
      "          24       0.73      0.57      0.64        14\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       0.92      0.79      0.85        14\n",
      "          27       0.56      1.00      0.71         5\n",
      "          28       0.94      0.89      0.91        18\n",
      "          29       0.94      0.58      0.71        26\n",
      "          30       1.00      0.50      0.67         2\n",
      "          31       1.00      0.50      0.67         2\n",
      "          32       0.75      0.60      0.67         5\n",
      "          33       0.90      1.00      0.95         9\n",
      "          34       0.67      0.67      0.67        48\n",
      "          35       0.80      0.80      0.80         5\n",
      "          36       1.00      1.00      1.00         4\n",
      "          37       0.92      0.75      0.83        16\n",
      "          38       0.67      0.40      0.50         5\n",
      "          39       0.67      0.67      0.67         6\n",
      "          40       1.00      0.50      0.67         4\n",
      "          41       0.00      0.00      0.00         6\n",
      "          42       0.89      0.81      0.85        21\n",
      "          43       0.80      0.80      0.80         5\n",
      "          44       0.82      0.82      0.82        11\n",
      "          45       0.67      0.40      0.50         5\n",
      "          46       0.78      0.64      0.70        11\n",
      "          47       0.92      0.85      0.88        26\n",
      "          48       0.50      0.33      0.40         3\n",
      "          49       0.50      1.00      0.67         5\n",
      "          50       0.50      0.50      0.50        20\n",
      "          51       0.92      0.73      0.81        15\n",
      "          52       1.00      0.50      0.67         8\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.83      0.81      0.82        43\n",
      "          55       0.61      0.79      0.69        14\n",
      "          56       0.88      0.96      0.92        23\n",
      "          57       0.88      0.70      0.78        10\n",
      "          58       0.94      0.80      0.86        20\n",
      "          59       1.00      0.40      0.57         5\n",
      "          60       1.00      0.75      0.86         4\n",
      "          61       0.50      0.20      0.29         5\n",
      "          62       1.00      1.00      1.00        19\n",
      "          63       0.80      0.88      0.84        60\n",
      "          64       0.57      0.57      0.57         7\n",
      "          65       0.67      0.70      0.68        23\n",
      "          66       0.71      0.91      0.80        33\n",
      "          67       1.00      1.00      1.00         3\n",
      "          68       0.90      0.98      0.94        47\n",
      "          69       0.38      0.75      0.50         4\n",
      "          70       0.67      1.00      0.80        16\n",
      "          71       1.00      0.90      0.95        10\n",
      "          72       1.00      1.00      1.00         1\n",
      "          73       0.83      0.62      0.71         8\n",
      "          74       0.64      0.58      0.61        12\n",
      "          75       1.00      1.00      1.00         2\n",
      "          76       0.90      0.86      0.88        21\n",
      "\n",
      "    accuracy                           0.78      1141\n",
      "   macro avg       0.80      0.73      0.75      1141\n",
      "weighted avg       0.80      0.78      0.78      1141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Decision Tree Classifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30],  \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4],  \n",
    "}\n",
    "\n",
    "# Initiate GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fitting model\n",
    "grid_search.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "best_model.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "y_pred_test = best_model.predict(X_test_tfidf)\n",
    "y_pred_valid = best_model.predict(X_valid_tfidf)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test_encoded, y_pred_test)\n",
    "report_test = classification_report(y_test_encoded, y_pred_test)\n",
    "\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "report_valid = classification_report(y_valid_encoded, y_pred_valid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test Report:\")\n",
    "print(report_test)\n",
    "print(\"Validation Report:\")\n",
    "print(report_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eef04dcf-0849-4a9a-b377-0a23980cf2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100, 'penalty': 'l2'}\n",
      "Test Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        24\n",
      "           1       0.86      0.92      0.89        13\n",
      "           2       0.96      1.00      0.98        25\n",
      "           3       0.88      0.88      0.88         8\n",
      "           4       1.00      0.93      0.96        27\n",
      "           5       0.91      1.00      0.95        10\n",
      "           6       0.91      0.94      0.93        53\n",
      "           7       0.98      0.95      0.96        42\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       0.90      0.75      0.82        12\n",
      "          10       0.91      0.91      0.91        11\n",
      "          11       0.82      0.80      0.81        41\n",
      "          12       0.96      0.92      0.94        50\n",
      "          13       0.94      1.00      0.97        17\n",
      "          14       0.98      0.94      0.96        47\n",
      "          15       0.99      0.95      0.97        82\n",
      "          16       0.58      0.64      0.61        11\n",
      "          17       0.92      0.94      0.93        48\n",
      "          18       0.85      0.97      0.91        36\n",
      "          19       0.83      0.97      0.89        30\n",
      "          20       0.94      0.94      0.94        16\n",
      "          21       1.00      1.00      1.00         4\n",
      "          22       1.00      0.25      0.40         4\n",
      "          23       0.89      0.67      0.76        12\n",
      "          24       0.95      0.90      0.93        21\n",
      "          25       1.00      0.94      0.97        16\n",
      "          26       0.95      0.95      0.95        21\n",
      "          27       1.00      1.00      1.00         8\n",
      "          28       1.00      0.96      0.98        28\n",
      "          29       0.90      0.90      0.90        39\n",
      "          30       1.00      1.00      1.00         4\n",
      "          31       0.00      0.00      0.00         3\n",
      "          32       1.00      1.00      1.00         7\n",
      "          33       0.93      0.93      0.93        14\n",
      "          34       0.96      0.99      0.97        71\n",
      "          35       1.00      0.88      0.93         8\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       0.82      0.92      0.87        25\n",
      "          38       1.00      1.00      1.00         7\n",
      "          39       1.00      0.89      0.94         9\n",
      "          40       1.00      0.50      0.67         6\n",
      "          41       0.75      0.33      0.46         9\n",
      "          42       0.91      0.97      0.94        31\n",
      "          43       1.00      0.86      0.92         7\n",
      "          44       0.74      0.88      0.80        16\n",
      "          45       0.67      0.86      0.75         7\n",
      "          46       0.82      0.82      0.82        17\n",
      "          47       0.95      0.97      0.96        38\n",
      "          48       1.00      1.00      1.00         5\n",
      "          49       0.89      1.00      0.94         8\n",
      "          50       0.90      0.93      0.92        29\n",
      "          51       1.00      0.96      0.98        23\n",
      "          52       1.00      0.64      0.78        11\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.95      0.97      0.96        63\n",
      "          55       0.87      0.91      0.89        22\n",
      "          56       0.92      0.97      0.94        35\n",
      "          57       1.00      0.94      0.97        16\n",
      "          58       1.00      1.00      1.00        30\n",
      "          59       0.71      0.62      0.67         8\n",
      "          60       1.00      0.86      0.92         7\n",
      "          61       1.00      1.00      1.00         8\n",
      "          62       0.97      1.00      0.98        29\n",
      "          63       0.89      0.96      0.92        90\n",
      "          64       0.50      0.55      0.52        11\n",
      "          65       0.91      0.94      0.93        34\n",
      "          66       0.88      0.86      0.87        49\n",
      "          67       0.75      0.75      0.75         4\n",
      "          68       0.95      0.99      0.97        71\n",
      "          69       1.00      1.00      1.00         6\n",
      "          70       0.92      0.88      0.90        25\n",
      "          71       0.93      1.00      0.97        14\n",
      "          72       1.00      1.00      1.00         2\n",
      "          73       0.91      0.83      0.87        12\n",
      "          74       0.94      0.88      0.91        17\n",
      "          75       1.00      0.50      0.67         2\n",
      "          76       1.00      0.94      0.97        31\n",
      "\n",
      "    accuracy                           0.92      1711\n",
      "   macro avg       0.90      0.86      0.87      1711\n",
      "weighted avg       0.92      0.92      0.92      1711\n",
      "\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        16\n",
      "           1       0.90      1.00      0.95         9\n",
      "           2       1.00      1.00      1.00        16\n",
      "           3       0.83      1.00      0.91         5\n",
      "           4       1.00      0.94      0.97        18\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.90      1.00      0.95        35\n",
      "           7       1.00      0.96      0.98        28\n",
      "           8       1.00      1.00      1.00         5\n",
      "           9       0.86      0.75      0.80         8\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       0.76      0.70      0.73        27\n",
      "          12       0.94      0.97      0.96        33\n",
      "          13       1.00      1.00      1.00        12\n",
      "          14       0.94      0.94      0.94        32\n",
      "          15       0.96      0.93      0.94        55\n",
      "          16       0.86      0.86      0.86         7\n",
      "          17       0.94      0.91      0.92        33\n",
      "          18       0.75      0.96      0.84        25\n",
      "          19       0.90      0.90      0.90        20\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         2\n",
      "          22       1.00      1.00      1.00         2\n",
      "          23       1.00      1.00      1.00         8\n",
      "          24       0.81      0.93      0.87        14\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       0.92      0.86      0.89        14\n",
      "          27       1.00      1.00      1.00         5\n",
      "          28       0.94      0.89      0.91        18\n",
      "          29       0.88      0.81      0.84        26\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      0.50      0.67         2\n",
      "          32       1.00      1.00      1.00         5\n",
      "          33       0.75      1.00      0.86         9\n",
      "          34       0.96      0.96      0.96        48\n",
      "          35       1.00      0.80      0.89         5\n",
      "          36       1.00      1.00      1.00         4\n",
      "          37       0.94      0.94      0.94        16\n",
      "          38       1.00      0.80      0.89         5\n",
      "          39       1.00      0.67      0.80         6\n",
      "          40       0.67      0.50      0.57         4\n",
      "          41       0.50      0.33      0.40         6\n",
      "          42       0.90      0.90      0.90        21\n",
      "          43       1.00      0.80      0.89         5\n",
      "          44       0.83      0.91      0.87        11\n",
      "          45       1.00      0.40      0.57         5\n",
      "          46       0.75      0.82      0.78        11\n",
      "          47       0.96      0.96      0.96        26\n",
      "          48       1.00      0.67      0.80         3\n",
      "          49       1.00      1.00      1.00         5\n",
      "          50       0.89      0.85      0.87        20\n",
      "          51       1.00      0.87      0.93        15\n",
      "          52       0.83      0.62      0.71         8\n",
      "          53       1.00      1.00      1.00         1\n",
      "          54       0.96      1.00      0.98        43\n",
      "          55       0.87      0.93      0.90        14\n",
      "          56       0.92      0.96      0.94        23\n",
      "          57       1.00      0.90      0.95        10\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       0.75      0.60      0.67         5\n",
      "          60       1.00      1.00      1.00         4\n",
      "          61       0.75      0.60      0.67         5\n",
      "          62       1.00      1.00      1.00        19\n",
      "          63       0.88      0.95      0.91        60\n",
      "          64       1.00      0.86      0.92         7\n",
      "          65       0.88      0.96      0.92        23\n",
      "          66       0.86      0.91      0.88        33\n",
      "          67       1.00      1.00      1.00         3\n",
      "          68       0.90      1.00      0.95        47\n",
      "          69       1.00      1.00      1.00         4\n",
      "          70       0.89      1.00      0.94        16\n",
      "          71       1.00      0.90      0.95        10\n",
      "          72       1.00      1.00      1.00         1\n",
      "          73       1.00      0.62      0.77         8\n",
      "          74       0.78      0.58      0.67        12\n",
      "          75       1.00      1.00      1.00         2\n",
      "          76       0.95      0.95      0.95        21\n",
      "\n",
      "    accuracy                           0.92      1141\n",
      "   macro avg       0.93      0.89      0.90      1141\n",
      "weighted avg       0.92      0.92      0.91      1141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  \n",
    "    'penalty': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_model = LogisticRegression(**best_params)\n",
    "\n",
    "best_model.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "y_pred_test = best_model.predict(X_test_tfidf)\n",
    "y_pred_valid = best_model.predict(X_valid_tfidf)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test_encoded, y_pred_test)\n",
    "report_test = classification_report(y_test_encoded, y_pred_test)\n",
    "\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "report_valid = classification_report(y_valid_encoded, y_pred_valid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test Report:\")\n",
    "print(report_test)\n",
    "print(\"Validation Report:\")\n",
    "print(report_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9042b31-82ed-41a7-be97-2b4f9c377d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "train_text_features = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "X_train_bow_normalized = normalize(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5ce54d3-485c-4f12-82db-788d6bc0b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['title'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d46ba97-aa6b-429d-9ae4-cd7886ef5267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b0cd7fe2-4559-4ad4-9763-8f2b1e05bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix After One Hot Encoding\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Matrix After One Hot Encoding\")\n",
    "print(X_train_bow_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45d43d20-8788-4a59-a052-1007c6f69b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF Matrix:\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\"Shape of TF-IDF Matrix:\")\n",
    "print(X_train_tfidf.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
